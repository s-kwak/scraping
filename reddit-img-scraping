import requests, requests.auth, json, pprint, bs4, os, time

# script inputs

limit = '25'
period = 'month'
subreddit = 'zzzzzz'

folderName = subreddit
os.makedirs(folderName, exist_ok=True)

# ---------------------
# authorization
# ---------------------

# making authorization request
clientId = 'zzzzzz'
clientSecret = 'zzzzzzz'
userAgent = 'Scraping Script by SK'

clientAuth = requests.auth.HTTPBasicAuth(clientId, clientSecret)

postData = {'grant_type': 'password', 'username': 'zzzzzz', 'password': 'zzzzz'}
headers = {'User-Agent': userAgent}
response = requests.post("https://www.reddit.com/api/v1/access_token",
                         auth=clientAuth, data=postData, headers=headers)

token = 'bearer ' + response.json()['access_token']

# using token
authHeaders = {"Authorization": token, "User-Agent": userAgent}
auth = requests.get("https://oauth.reddit.com/api/v1/me", headers=authHeaders)

# ---------------------
# downloading
# ---------------------

# get json
jsonUrl = 'https://www.reddit.com/r/' + subreddit + '/top.json?limit=' + limit + '&t=' + period
jsonContent = requests.get(jsonUrl)
jsonAsPython = json.loads(jsonContent.text)

# DOESNT DOWNLOAD GIFV WITHIN IMGUR ALBUMS PROPERLY
# ALSO DOESNT DOWNLOAD SINGLE IMAGE FILES IF ZOOMED OR ALBUMS IF NONZOOMED

for i in range(0, len(jsonAsPython['data']['children'])):
    linkTitle = jsonAsPython['data']['children'][i]['data']['title']
    linkContentUrl = jsonAsPython['data']['children'][i]['data']['url']
    linkContentScore = jsonAsPython['data']['children'][i]['data']['score']
    print('[' + str(linkContentScore) + '] ' + linkTitle[:50])
    # direct link image files
    # if linkContentUrl[-3:] in ['jpg', 'peg', 'png', 'gif']:
    if max(linkContentUrl.find('.jpg'), linkContentUrl.find('.jpeg'), linkContentUrl.find('.png'), linkContentUrl.find('.gif')) >= 0 and linkContentUrl.find('.gifv') == -1:
        if linkContentUrl.find('?') > -1:
            linkContentUrl = linkContentUrl[:-2]
        else:
            linkContentUrl = linkContentUrl
        res_content = requests.get(linkContentUrl)
        dlFile = open(os.path.join(folderName, os.path.basename(linkContentUrl)), 'wb')
        for chunk in res_content.iter_content(100000):
            dlFile.write(chunk)
        dlFile.close()
        print('  DOWNLOADED ' + str(i+1) + ' of ' + str(len(jsonAsPython['data']['children'])))
    # gifv files
    elif linkContentUrl[-4:] in ['gifv']:
        res_content = requests.get(linkContentUrl[:-1])
        dlFile = open(os.path.join(folderName, os.path.basename(linkContentUrl[:-1])), 'wb')
        for chunk in res_content.iter_content(100000):
            dlFile.write(chunk)
        dlFile.close()
        print('  DOWNLOADED ' + str(i+1) + ' of ' + str(len(jsonAsPython['data']['children'])))
    # imgur albums (only visible images)
    elif jsonAsPython['data']['children'][i]['data']['url'].find('gallery') > 0 or jsonAsPython['data']['children'][i]['data']['url'].find('/a/') > 0:
        res = requests.get(linkContentUrl)
        soup = bs4.BeautifulSoup(res.text, 'lxml')
        images = soup.find_all('a', class_='zoom')
        for j in range(len(images)):
            imgUrl = 'https://' + images[j].get('href')[2:]
            res_content = requests.get(imgUrl)
            dlFile = open(os.path.join(folderName, os.path.basename(imgUrl)), 'wb')
            for chunk in res_content.iter_content(100000):
                dlFile.write(chunk)
            dlFile.close()
        print('  DOWNLOADED ' + str(i+1) + ' of ' + str(len(jsonAsPython['data']['children'])))
    # imgur single image links
    elif jsonAsPython['data']['children'][i]['data']['domain'] == 'i.imgur.com':
        res = requests.get(linkContentUrl)
        soup = bs4.BeautifulSoup(res.text, 'lxml')
        images = soup.find_all('div', class_='post-image')[0].find_all('img')[0].get('src')[2:]
        imgUrl = 'https://' + images
        res_content = requests.get(imgUrl)
        dlFile = open(os.path.join(folderName, os.path.basename(imgUrl)), 'wb')
        for chunk in res_content.iter_content(100000):
            dlFile.write(chunk)
        dlFile.close()
        print('  DOWNLOADED ' + str(i+1) + ' of ' + str(len(jsonAsPython['data']['children'])))
    # gfycat links
    elif jsonAsPython['data']['children'][i]['data']['domain'] == 'gfycat.com':
        res = requests.get(linkContentUrl)
        soup = bs4.BeautifulSoup(res.text, 'lxml')
        gfycatFile = soup.select('#main-container source')[1].get('src')
        res_content = requests.get(gfycatFile)
        dlFile = open(os.path.join(folderName, os.path.basename(gfycatFile)), 'wb')
        for chunk in res_content.iter_content(100000):
            dlFile.write(chunk)
        dlFile.close()
        print('  DOWNLOADED ' + str(i+1) + ' of ' + str(len(jsonAsPython['data']['children'])))
    # else
    else:
        print('  COULD NOT DOWNLOADED ' + str(i+1) + ' of ' + str(len(jsonAsPython['data']['children'])))
